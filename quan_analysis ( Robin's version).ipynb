{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d7bb409",
   "metadata": {},
   "source": [
    "# Robin Nguyen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381266ed",
   "metadata": {},
   "source": [
    "# I. Insert data \n",
    "## 1. Log Daily Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8061cc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "335e5d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate Yahoo Finance Data\n",
    "def dret(ticker, start_date=\"2018-01-01\", end_date=\"2024-12-31\", output_directory=output_dir):\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    # Download price data\n",
    "    pricedata = yf.download(ticker, start=start_date, end=end_date, progress=False)\n",
    "\n",
    "    if pricedata.empty:\n",
    "        return None\n",
    "\n",
    "    # Suppress RuntimeWarning: invalid value encountered in log\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "        \n",
    "        # Calculate daily log returns using the 'Close' column\n",
    "        pricedata[('Daily Return', ticker)] = np.log(pricedata['Close'] / pricedata['Close'].shift(1))\n",
    "\n",
    "    # Drop NaN values resulting from the shift\n",
    "    pricedata = pricedata.dropna()\n",
    "\n",
    "    # Define the output file path\n",
    "    output_file = f\"{output_directory}/{ticker}.csv\"\n",
    "\n",
    "    # Check if the file exists and delete it before writing\n",
    "    if os.path.exists(output_file):\n",
    "        os.remove(output_file)  # Delete the file to avoid permission issues\n",
    "\n",
    "    # Save the data to the file\n",
    "    pricedata.to_csv(output_file, index=True)\n",
    "\n",
    "    return pricedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bc991fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YF.download() has changed argument auto_adjust default to True\n"
     ]
    }
   ],
   "source": [
    "# Define the directory path for saving data\n",
    "output_dir_tech = \"./data/quantitative/tech\"\n",
    "\n",
    "# List of stock tickers\n",
    "tech_tickers = [\n",
    "    \"AAPL\", \"MSFT\", \"GOOGL\", \"GOOG\", \"AMZN\", \"META\", \"NVDA\", \"TSM\", \"ADBE\", \"INTC\",\n",
    "    \"CSCO\", \"ORCL\", \"IBM\", \"CRM\", \"QCOM\", \"AVGO\", \"TXN\", \"AMD\", \"AMAT\", \"MU\",\n",
    "    \"NET\", \"NOW\", \"SNOW\", \"DOCU\", \"SHOP\", \"UBER\", \"LYFT\", \"SNAP\", \"HRB\", \"DDOG\"\n",
    "]\n",
    "\n",
    "# Download data for each ticker\n",
    "for ticker in tech_tickers:\n",
    "    data_tech = dret(ticker, start_date=\"2018-01-01\", end_date=\"2024-12-31\", output_directory=output_dir_tech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf45c076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory path for saving data\n",
    "output_dir_finbank = \"./data/quantitative/fin_bank\"\n",
    "\n",
    "# List of financial and banking tickers\n",
    "finbank_tickers = [\n",
    "    \"JPM\", \"BAC\", \"WFC\", \"C\", \"GS\", \"MS\", \"USB\", \"PNC\", \"TFC\", \"COF\",\n",
    "    \"TD\", \"SCHW\", \"BK\", \"STT\", \"AXP\", \"HSBC\", \"CFG\", \"FITB\", \"MTB\", \"HBAN\",\n",
    "    \"ALLY\", \"KEY\", \"RY\", \"SAN\", \"NTRS\", \"RF\", \"SYF\", \"NBHC\", \"ZION\", \"FHN\"\n",
    "]\n",
    "\n",
    "# Download data for each ticker\n",
    "for ticker in finbank_tickers:\n",
    "    data_finbank = dret(ticker, start_date=\"2018-01-01\", end_date=\"2024-12-31\", output_directory=output_dir_finbank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9c46303",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the directory path for saving data\n",
    "output_dir_ev = \"./data/quantitative/ev\"\n",
    "\n",
    "# List of financial and banking tickers\n",
    "ev_tickers = [\n",
    "    \"TSLA\", \"BYDDY\", \"LI\", \"NIO\", \"RIVN\", \"LCID\", \"XPEV\", \"NKLA\", \"PSNY\", \"GM\", \"F\",\n",
    "    \"VWAGY\", \"BAMXF\", \"HYMTF\", \"KIMTF\", \"POAHY\", \"MBGYY\", \"STLA\", \"GELYF\", \"GWLLY\",\n",
    "    \"SAIC\", \"HYLN\", \"GNZUF\", \"TATAMOTORS.NS\", \"MAHMF\", \"RNLSY\", \"NSANY\", \"MMTOF\"\n",
    "]\n",
    "\n",
    "# Download data for each ticker\n",
    "for ticker in ev_tickers:\n",
    "    data_ev = dret(ticker, start_date=\"2018-01-01\", end_date=\"2024-12-31\", output_directory=output_dir_ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89392762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory path for saving data\n",
    "output_dir_market = \"./data/quantitative\"\n",
    "\n",
    "market_data = dret(\"^GSPC\", start_date=\"2018-01-01\", end_date=\"2024-12-31\", output_directory=output_dir_market)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fc5cc39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Daily Return</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>^GSPC</th>\n",
       "      <th>^GSPC</th>\n",
       "      <th>^GSPC</th>\n",
       "      <th>^GSPC</th>\n",
       "      <th>^GSPC</th>\n",
       "      <th>^GSPC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>2713.060059</td>\n",
       "      <td>2714.370117</td>\n",
       "      <td>2697.770020</td>\n",
       "      <td>2697.850098</td>\n",
       "      <td>3544030000</td>\n",
       "      <td>0.006378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>2723.989990</td>\n",
       "      <td>2729.290039</td>\n",
       "      <td>2719.070068</td>\n",
       "      <td>2719.310059</td>\n",
       "      <td>3697340000</td>\n",
       "      <td>0.004021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>2743.149902</td>\n",
       "      <td>2743.449951</td>\n",
       "      <td>2727.919922</td>\n",
       "      <td>2731.330078</td>\n",
       "      <td>3239280000</td>\n",
       "      <td>0.007009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>2747.709961</td>\n",
       "      <td>2748.510010</td>\n",
       "      <td>2737.600098</td>\n",
       "      <td>2742.669922</td>\n",
       "      <td>3246160000</td>\n",
       "      <td>0.001661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-09</th>\n",
       "      <td>2751.290039</td>\n",
       "      <td>2759.139893</td>\n",
       "      <td>2747.860107</td>\n",
       "      <td>2751.149902</td>\n",
       "      <td>3467460000</td>\n",
       "      <td>0.001302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-23</th>\n",
       "      <td>5974.069824</td>\n",
       "      <td>5978.250000</td>\n",
       "      <td>5902.569824</td>\n",
       "      <td>5940.250000</td>\n",
       "      <td>3593280000</td>\n",
       "      <td>0.007261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-24</th>\n",
       "      <td>6040.040039</td>\n",
       "      <td>6040.100098</td>\n",
       "      <td>5981.439941</td>\n",
       "      <td>5984.629883</td>\n",
       "      <td>1757720000</td>\n",
       "      <td>0.010982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-26</th>\n",
       "      <td>6037.589844</td>\n",
       "      <td>6049.750000</td>\n",
       "      <td>6007.370117</td>\n",
       "      <td>6024.970215</td>\n",
       "      <td>2904530000</td>\n",
       "      <td>-0.000406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-27</th>\n",
       "      <td>5970.839844</td>\n",
       "      <td>6006.169922</td>\n",
       "      <td>5932.950195</td>\n",
       "      <td>6006.169922</td>\n",
       "      <td>3159610000</td>\n",
       "      <td>-0.011117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-30</th>\n",
       "      <td>5906.939941</td>\n",
       "      <td>5940.790039</td>\n",
       "      <td>5869.160156</td>\n",
       "      <td>5920.669922</td>\n",
       "      <td>3433250000</td>\n",
       "      <td>-0.010760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1759 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Price             Close         High          Low         Open      Volume  \\\n",
       "Ticker            ^GSPC        ^GSPC        ^GSPC        ^GSPC       ^GSPC   \n",
       "Date                                                                         \n",
       "2018-01-03  2713.060059  2714.370117  2697.770020  2697.850098  3544030000   \n",
       "2018-01-04  2723.989990  2729.290039  2719.070068  2719.310059  3697340000   \n",
       "2018-01-05  2743.149902  2743.449951  2727.919922  2731.330078  3239280000   \n",
       "2018-01-08  2747.709961  2748.510010  2737.600098  2742.669922  3246160000   \n",
       "2018-01-09  2751.290039  2759.139893  2747.860107  2751.149902  3467460000   \n",
       "...                 ...          ...          ...          ...         ...   \n",
       "2024-12-23  5974.069824  5978.250000  5902.569824  5940.250000  3593280000   \n",
       "2024-12-24  6040.040039  6040.100098  5981.439941  5984.629883  1757720000   \n",
       "2024-12-26  6037.589844  6049.750000  6007.370117  6024.970215  2904530000   \n",
       "2024-12-27  5970.839844  6006.169922  5932.950195  6006.169922  3159610000   \n",
       "2024-12-30  5906.939941  5940.790039  5869.160156  5920.669922  3433250000   \n",
       "\n",
       "Price      Daily Return  \n",
       "Ticker            ^GSPC  \n",
       "Date                     \n",
       "2018-01-03     0.006378  \n",
       "2018-01-04     0.004021  \n",
       "2018-01-05     0.007009  \n",
       "2018-01-08     0.001661  \n",
       "2018-01-09     0.001302  \n",
       "...                 ...  \n",
       "2024-12-23     0.007261  \n",
       "2024-12-24     0.010982  \n",
       "2024-12-26    -0.000406  \n",
       "2024-12-27    -0.011117  \n",
       "2024-12-30    -0.010760  \n",
       "\n",
       "[1759 rows x 6 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View data\n",
    "market_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b286df9",
   "metadata": {},
   "source": [
    "## 2. FAMA-FERNCH 3-FACTOR MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e1eddf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fama_french_df = pd.read_csv(r\"C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\F-F_Research_Data_Factors_daily.CSV\",header=0, skiprows=3, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12440cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Mkt-RF</th>\n",
       "      <th>SMB</th>\n",
       "      <th>HML</th>\n",
       "      <th>RF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19260701</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19260702</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19260706</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19260707</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19260708</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25897</th>\n",
       "      <td>20241226</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.04</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25898</th>\n",
       "      <td>20241227</td>\n",
       "      <td>-1.17</td>\n",
       "      <td>-0.66</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25899</th>\n",
       "      <td>20241230</td>\n",
       "      <td>-1.09</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25900</th>\n",
       "      <td>20241231</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25901</th>\n",
       "      <td>Copyright 2024 Eugene F. Fama and Kenneth R. F...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25902 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Unnamed: 0  Mkt-RF   SMB   HML  \\\n",
       "0                                               19260701    0.10 -0.25 -0.27   \n",
       "1                                               19260702    0.45 -0.33 -0.06   \n",
       "2                                               19260706    0.17  0.30 -0.39   \n",
       "3                                               19260707    0.09 -0.58  0.02   \n",
       "4                                               19260708    0.21 -0.38  0.19   \n",
       "...                                                  ...     ...   ...   ...   \n",
       "25897                                           20241226    0.02  1.04 -0.19   \n",
       "25898                                           20241227   -1.17 -0.66  0.56   \n",
       "25899                                           20241230   -1.09  0.12  0.74   \n",
       "25900                                           20241231   -0.46  0.00  0.71   \n",
       "25901  Copyright 2024 Eugene F. Fama and Kenneth R. F...     NaN   NaN   NaN   \n",
       "\n",
       "          RF  \n",
       "0      0.009  \n",
       "1      0.009  \n",
       "2      0.009  \n",
       "3      0.009  \n",
       "4      0.009  \n",
       "...      ...  \n",
       "25897  0.017  \n",
       "25898  0.017  \n",
       "25899  0.017  \n",
       "25900  0.017  \n",
       "25901    NaN  \n",
       "\n",
       "[25902 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view dataframe\n",
    "fama_french_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3394a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Date    Mkt-RF       SMB       HML            RF\n",
      "24140 2018-01-02  0.000085  0.000035 -0.000022  6.000000e-07\n",
      "24141 2018-01-03  0.000059 -0.000039 -0.000018  6.000000e-07\n",
      "24142 2018-01-04  0.000042 -0.000026  0.000024  6.000000e-07\n",
      "24143 2018-01-05  0.000066 -0.000036 -0.000026  6.000000e-07\n",
      "24144 2018-01-08  0.000019 -0.000015  0.000004  6.000000e-07\n",
      "Cleaned data has been saved to: C:\\\\Users\\\\haloi\\\\Downloads\\\\data_201_proj\\\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\\\data\\\\quantitative\\\\fama_french\\\\Cleaned_Fama_French_Data.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Rename the 'Unnamed: 0' column to 'Date'\n",
    "fama_french_df = fama_french_df.rename(columns={'Unnamed: 0': 'Date'})\n",
    "\n",
    "# Remove the copyright row at the end of the table\n",
    "fama_french_df = fama_french_df.iloc[:-1, :]\n",
    "\n",
    "# Convert the 'Date' column to the appropriate datetime format\n",
    "fama_french_df['Date'] = pd.to_datetime(fama_french_df['Date'], format='%Y%m%d')\n",
    "\n",
    "# Convert percentage columns to decimal numbers\n",
    "numeric_columns = ['Mkt-RF', 'SMB', 'HML', 'RF']\n",
    "fama_french_df[numeric_columns] = fama_french_df[numeric_columns].astype(float) / 100\n",
    "\n",
    "# Filter the data for the specified date range\n",
    "start_date = pd.Timestamp('2018-01-01')\n",
    "end_date = pd.Timestamp('2024-12-31')\n",
    "fama_french_df = fama_french_df.loc[(fama_french_df['Date'] >= start_date) & (fama_french_df['Date'] <= end_date)]\n",
    "\n",
    "# Display the first few rows of the filtered DataFrame\n",
    "print(fama_french_df.head())\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "output_file = r\"C:\\\\Users\\\\haloi\\\\Downloads\\\\data_201_proj\\\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\\\data\\\\quantitative\\\\fama_french\\\\Cleaned_Fama_French_Data.csv\"\n",
    "fama_french_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Cleaned data has been saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b21472d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mkt-RF</th>\n",
       "      <th>SMB</th>\n",
       "      <th>HML</th>\n",
       "      <th>RF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1761.000000</td>\n",
       "      <td>1761.000000</td>\n",
       "      <td>1761.000000</td>\n",
       "      <td>1761.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.000488</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>-0.000121</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.012738</td>\n",
       "      <td>0.007145</td>\n",
       "      <td>0.010180</td>\n",
       "      <td>0.000078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.120000</td>\n",
       "      <td>-0.035600</td>\n",
       "      <td>-0.049700</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.004800</td>\n",
       "      <td>-0.004400</td>\n",
       "      <td>-0.005500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000800</td>\n",
       "      <td>-0.000300</td>\n",
       "      <td>-0.000500</td>\n",
       "      <td>0.000070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>0.000170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.093400</td>\n",
       "      <td>0.054700</td>\n",
       "      <td>0.067300</td>\n",
       "      <td>0.000220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Mkt-RF          SMB          HML           RF\n",
       "count  1761.000000  1761.000000  1761.000000  1761.000000\n",
       "mean      0.000488    -0.000078    -0.000121     0.000089\n",
       "std       0.012738     0.007145     0.010180     0.000078\n",
       "min      -0.120000    -0.035600    -0.049700     0.000000\n",
       "25%      -0.004800    -0.004400    -0.005500     0.000000\n",
       "50%       0.000800    -0.000300    -0.000500     0.000070\n",
       "75%       0.007100     0.004100     0.005200     0.000170\n",
       "max       0.093400     0.054700     0.067300     0.000220"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary satistic\n",
    "fama_french_df.iloc[:, 1:5].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "a1458c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values:\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "missing_values = fama_french_df.isna().sum()\n",
    "print(\"Missing Values:\")\n",
    "print(missing_values[missing_values > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4532d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data for AAPL.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_AAPL.csv\n",
      "Merged data for ADBE.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_ADBE.csv\n",
      "Merged data for AMAT.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_AMAT.csv\n",
      "Merged data for AMD.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_AMD.csv\n",
      "Merged data for AMZN.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_AMZN.csv\n",
      "Merged data for AVGO.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_AVGO.csv\n",
      "Merged data for BAMXF.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_BAMXF.csv\n",
      "Merged data for BYDDY.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_BYDDY.csv\n",
      "Merged data for CRM.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_CRM.csv\n",
      "Merged data for CSCO.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_CSCO.csv\n",
      "Merged data for DDOG.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_DDOG.csv\n",
      "Merged data for DOCU.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_DOCU.csv\n",
      "Merged data for F.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_F.csv\n",
      "Merged data for GELYF.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_GELYF.csv\n",
      "Merged data for GM.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_GM.csv\n",
      "Merged data for GNZUF.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_GNZUF.csv\n",
      "Merged data for GOOG.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_GOOG.csv\n",
      "Merged data for GOOGL.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_GOOGL.csv\n",
      "Merged data for GWLLY.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_GWLLY.csv\n",
      "Merged data for HRB.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_HRB.csv\n",
      "Merged data for HYLN.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_HYLN.csv\n",
      "Merged data for HYMTF.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_HYMTF.csv\n",
      "Merged data for IBM.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_IBM.csv\n",
      "Merged data for INTC.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_INTC.csv\n",
      "Merged data for KIMTF.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_KIMTF.csv\n",
      "Merged data for LCID.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_LCID.csv\n",
      "Merged data for LI.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_LI.csv\n",
      "Merged data for LYFT.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_LYFT.csv\n",
      "Merged data for MAHMF.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_MAHMF.csv\n",
      "Merged data for MBGYY.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_MBGYY.csv\n",
      "Merged data for META.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_META.csv\n",
      "Merged data for MMTOF.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_MMTOF.csv\n",
      "Merged data for MSFT.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_MSFT.csv\n",
      "Merged data for MU.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_MU.csv\n",
      "Merged data for NET.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_NET.csv\n",
      "Merged data for NIO.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_NIO.csv\n",
      "Merged data for NKLA.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_NKLA.csv\n",
      "Merged data for NOW.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_NOW.csv\n",
      "Merged data for NSANY.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_NSANY.csv\n",
      "Merged data for NVDA.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_NVDA.csv\n",
      "Merged data for ORCL.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_ORCL.csv\n",
      "Merged data for POAHY.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_POAHY.csv\n",
      "Merged data for PSNY.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_PSNY.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data for QCOM.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_QCOM.csv\n",
      "Merged data for RIVN.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_RIVN.csv\n",
      "Merged data for RNLSY.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_RNLSY.csv\n",
      "Merged data for SAIC.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_SAIC.csv\n",
      "Merged data for SHOP.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_SHOP.csv\n",
      "Merged data for SNAP.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_SNAP.csv\n",
      "Merged data for SNOW.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_SNOW.csv\n",
      "Merged data for STLA.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_STLA.csv\n",
      "Merged data for TATAMOTORS.NS.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_TATAMOTORS.NS.csv\n",
      "Merged data for TSLA.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_TSLA.csv\n",
      "Merged data for TSM.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_TSM.csv\n",
      "Merged data for TXN.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_TXN.csv\n",
      "Merged data for UBER.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_UBER.csv\n",
      "Merged data for VWAGY.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_VWAGY.csv\n",
      "Merged data for XPEV.csv saved to: C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\\merged_XPEV.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "stock_data_dir = r\"C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\ev\"\n",
    "fama_french_file_path = r\"C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\Cleaned_Fama_French_Data.csv\"\n",
    "output_dir = r\"C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\"\n",
    "\n",
    "# Load the Fama-French data\n",
    "fama_french_df = pd.read_csv(fama_french_file_path)\n",
    "fama_french_df['Date'] = pd.to_datetime(fama_french_df['Date'])  # Ensure 'Date' is in datetime format\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process each stock data file in the stock data directory\n",
    "for file_name in os.listdir(stock_data_dir):\n",
    "    if file_name.endswith('.csv'):  # Ensure it's a CSV file\n",
    "        stock_file_path = os.path.join(stock_data_dir, file_name)\n",
    "\n",
    "        try:\n",
    "            # Load the stock data with the first three rows as header levels\n",
    "            stock_df = pd.read_csv(stock_file_path, header=[0, 1, 2])  # Include third row for column names\n",
    "            \n",
    "            # Flatten the multi-level columns\n",
    "            stock_df.columns = stock_df.columns.map(lambda x: x[2])  # Use the third level of the header for column names\n",
    "            \n",
    "            # Rename the columns\n",
    "            new_column_names = ['Date', 'Close', 'High', 'Low', 'Open', 'Volume', 'Daily Return']\n",
    "            if len(stock_df.columns) == len(new_column_names):\n",
    "                stock_df.columns = new_column_names\n",
    "            else:\n",
    "                print(f\"Column count mismatch in {file_name}: Skipping file.\")\n",
    "                continue  # Skip this file if column count doesn't match\n",
    "            \n",
    "            # Ensure 'Date' is in datetime format\n",
    "            stock_df['Date'] = pd.to_datetime(stock_df['Date'])\n",
    "            \n",
    "            # Merge stock data with Fama-French factors\n",
    "            merged_df = pd.merge(stock_df, fama_french_df, on='Date', how='inner')\n",
    "            \n",
    "            # Save the merged DataFrame to the output directory\n",
    "            output_file_path = os.path.join(output_dir, f\"merged_{file_name}\")\n",
    "            merged_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "            print(f\"Merged data for {file_name} saved to: {output_file_path}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df90955a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: merged_AAPL.csv\n",
      "Processing file: merged_ADBE.csv\n",
      "Processing file: merged_AMAT.csv\n",
      "Processing file: merged_AMD.csv\n",
      "Processing file: merged_AMZN.csv\n",
      "Processing file: merged_AVGO.csv\n",
      "Processing file: merged_BAMXF.csv\n",
      "Processing file: merged_BYDDY.csv\n",
      "Processing file: merged_CRM.csv\n",
      "Processing file: merged_CSCO.csv\n",
      "Processing file: merged_DDOG.csv\n",
      "Processing file: merged_DOCU.csv\n",
      "Processing file: merged_F.csv\n",
      "Processing file: merged_GELYF.csv\n",
      "Processing file: merged_GM.csv\n",
      "Processing file: merged_GNZUF.csv\n",
      "Processing file: merged_GOOG.csv\n",
      "Processing file: merged_GOOGL.csv\n",
      "Processing file: merged_GWLLY.csv\n",
      "Processing file: merged_HRB.csv\n",
      "Processing file: merged_HYLN.csv\n",
      "Processing file: merged_HYMTF.csv\n",
      "Processing file: merged_IBM.csv\n",
      "Processing file: merged_INTC.csv\n",
      "Processing file: merged_KIMTF.csv\n",
      "Processing file: merged_LCID.csv\n",
      "Processing file: merged_LI.csv\n",
      "Processing file: merged_LYFT.csv\n",
      "Processing file: merged_MAHMF.csv\n",
      "Processing file: merged_MBGYY.csv\n",
      "Processing file: merged_META.csv\n",
      "Processing file: merged_MMTOF.csv\n",
      "Processing file: merged_MSFT.csv\n",
      "Processing file: merged_MU.csv\n",
      "Processing file: merged_NET.csv\n",
      "Processing file: merged_NIO.csv\n",
      "Processing file: merged_NKLA.csv\n",
      "Processing file: merged_NOW.csv\n",
      "Processing file: merged_NSANY.csv\n",
      "Processing file: merged_NVDA.csv\n",
      "Processing file: merged_ORCL.csv\n",
      "Processing file: merged_POAHY.csv\n",
      "Processing file: merged_PSNY.csv\n",
      "Processing file: merged_QCOM.csv\n",
      "Processing file: merged_RIVN.csv\n",
      "Processing file: merged_RNLSY.csv\n",
      "Processing file: merged_SAIC.csv\n",
      "Processing file: merged_SHOP.csv\n",
      "Processing file: merged_SNAP.csv\n",
      "Processing file: merged_SNOW.csv\n",
      "Processing file: merged_STLA.csv\n",
      "Processing file: merged_TATAMOTORS.NS.csv\n",
      "Processing file: merged_TSLA.csv\n",
      "Processing file: merged_TSM.csv\n",
      "Processing file: merged_TXN.csv\n",
      "Processing file: merged_UBER.csv\n",
      "Processing file: merged_VWAGY.csv\n",
      "Processing file: merged_XPEV.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Define the path to the directory with merged files\n",
    "merged_data_dir = r\"C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\"\n",
    "\n",
    "# Loop through all merged CSV files in the directory\n",
    "for file_name in os.listdir(merged_data_dir):\n",
    "    if file_name.startswith(\"merged_\") and file_name.endswith(\".csv\"):  # Process only merged files\n",
    "        merged_file_path = os.path.join(merged_data_dir, file_name)\n",
    "\n",
    "        try:\n",
    "            # Load the merged CSV file\n",
    "            df = pd.read_csv(merged_file_path)\n",
    "            print(f\"Processing file: {file_name}\")\n",
    "\n",
    "            # Ensure 'Date' is in datetime format\n",
    "            df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "            # Calculate y (dependent variable): Daily Return - RF\n",
    "            df['y'] = df['Daily Return'] - df['RF']\n",
    "\n",
    "            # Define x (independent variables): Mkt-RF, SMB, HML\n",
    "            x = df[['Mkt-RF', 'SMB', 'HML']]\n",
    "            y = df['y']\n",
    "\n",
    "            # Add a constant term to the independent variables for the intercept\n",
    "            x = sm.add_constant(x)\n",
    "\n",
    "            # Perform the regression\n",
    "            model = sm.OLS(y, x).fit()\n",
    "\n",
    "            # Predicted values\n",
    "            y_pred = model.predict(x)\n",
    "\n",
    "            # Calculate verification metrics\n",
    "            mae = mean_absolute_error(y, y_pred)\n",
    "            mse = mean_squared_error(y, y_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "\n",
    "            # Save the regression summary and metrics to a text file\n",
    "            output_summary_path = os.path.join(merged_data_dir, f\"fama_french_model_summary_{file_name.replace('merged_', '').replace('.csv', '')}.txt\")\n",
    "            with open(output_summary_path, \"w\") as f:\n",
    "                f.write(model.summary().as_text())\n",
    "                f.write(\"\\n\\nVerification Metrics:\\n\")\n",
    "                f.write(f\"Mean Absolute Error (MAE): {mae:.6f}\\n\")\n",
    "                f.write(f\"Mean Squared Error (MSE): {mse:.6f}\\n\")\n",
    "                f.write(f\"Root Mean Squared Error (RMSE): {rmse:.6f}\\n\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d1e2c18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Metrics for TSLA ---\n",
      "Mean Absolute Error (MAE): 0.023200\n",
      "Mean Squared Error (MSE): 0.001112\n",
      "Root Mean Squared Error (RMSE): 0.033352\n",
      "\n",
      "--- Metrics for AAPL ---\n",
      "Mean Absolute Error (MAE): 0.008069\n",
      "Mean Squared Error (MSE): 0.000129\n",
      "Root Mean Squared Error (RMSE): 0.011343\n",
      "\n",
      "--- Metrics for AMZN ---\n",
      "Mean Absolute Error (MAE): 0.009660\n",
      "Mean Squared Error (MSE): 0.000201\n",
      "Root Mean Squared Error (RMSE): 0.014161\n",
      "\n",
      "--- Metrics for ADBE ---\n",
      "Mean Absolute Error (MAE): 0.009241\n",
      "Mean Squared Error (MSE): 0.000218\n",
      "Root Mean Squared Error (RMSE): 0.014754\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the path to the directory containing the .txt files\n",
    "results_dir = r\"C:\\Users\\haloi\\Downloads\\data_201_proj\\Forecasting-Stock-Perfomance-using-Sentiment-Analysis-ARIMAX-RNN-and-LSTM-main\\data\\quantitative\\fama_french\\ev\"\n",
    "\n",
    "# Stocks to extract metrics for\n",
    "specific_stocks = ['TSLA', 'AAPL', 'AMZN', 'ADBE']\n",
    "\n",
    "# Loop through the specific stocks\n",
    "for stock in specific_stocks:\n",
    "    # Construct the file name for each stock\n",
    "    file_path = os.path.join(results_dir, f\"fama_french_model_summary_{stock}.txt\")\n",
    "\n",
    "    # Check if the file exists\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"\\n--- Metrics for {stock} ---\")\n",
    "        try:\n",
    "            # Open and read the file\n",
    "            with open(file_path, 'r') as file:\n",
    "                lines = file.readlines()\n",
    "\n",
    "            # Search for the metrics in the file\n",
    "            for line in lines:\n",
    "                if \"Mean Absolute Error (MAE):\" in line:\n",
    "                    print(line.strip())\n",
    "                elif \"Mean Squared Error (MSE):\" in line:\n",
    "                    print(line.strip())\n",
    "                elif \"Root Mean Squared Error (RMSE):\" in line:\n",
    "                    print(line.strip())\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file for {stock}: {e}\")\n",
    "    else:\n",
    "        print(f\"File not found for {stock}: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476f31a2",
   "metadata": {},
   "source": [
    "## - TSLA: The MAE of 0.0232 indicates that, on average, the model's predictions deviate from the true values by approximately 2.32%. The MSE and RMSE highlight that larger errors are present, as the RMSE (0.0334) is significantly higher than the MAE. This suggests that the model struggles with capturing extreme deviations in TSLA's stock performance.\n",
    "\n",
    "## - AAPL:  The MAE of 0.0081 shows that the model's predictions are highly accurate, with an average error of less than 1%. The MSE (0.000129) and RMSE (0.0113) confirm strong performance, as the small values indicate the model handles both average and extreme errors well. Overall, the model performs exceptionally well for AAPL.\n",
    "\n",
    "## - ADBE: The MAE of 0.0097 indicates that the model's predictions for AMZN deviate by less than 1% on average. The MSE (0.000201) and RMSE (0.0142) suggest that while the model has minor errors, it performs slightly worse than it does for AAPL. However, the overall accuracy is still satisfactory.\n",
    "\n",
    "## - AMZN : The MAE of 0.0092 reflects that the model's average prediction error for ADBE is just below 1%. The MSE (0.000218) and RMSE (0.0148) suggest that the model performs slightly less effectively compared to AAPL and AMZN. While ADBE's metrics show reasonable accuracy, the slightly higher RMSE indicates occasional larger prediction errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a15af8",
   "metadata": {},
   "source": [
    "# II. CAPM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e1a3ff19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acbfc50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CAPM function\n",
    "def perform_capm(stock_tickers, market_data, output_dir, start_date=\"2018-01-01\", end_date=\"2024-12-31\"):\n",
    "    stock_data = {}\n",
    "    capm_results = {}\n",
    "\n",
    "    # Download stock data for each ticker\n",
    "    for ticker in stock_tickers:\n",
    "        stock_data[ticker] = dret(ticker, start_date=start_date, end_date=end_date, output_directory=output_dir)\n",
    "\n",
    "    # Ensure market data is available\n",
    "    if market_data is not None:\n",
    "        # Independent variable: market daily returns\n",
    "        market_returns = market_data['Daily Return']\n",
    "\n",
    "        # Perform CAPM regression for each stock\n",
    "        for ticker, data in stock_data.items():\n",
    "            if data is not None:\n",
    "                try:\n",
    "                    # Dependent variable: stock daily returns\n",
    "                    stock_returns = data['Daily Return']\n",
    "\n",
    "                    # Align data to avoid NaN issues\n",
    "                    combined_data = pd.concat([stock_returns, market_returns], axis=1).dropna()\n",
    "                    stock_returns_aligned = combined_data.iloc[:, 0]\n",
    "                    market_returns_aligned = combined_data.iloc[:, 1]\n",
    "\n",
    "                    # Add a constant to the independent variable\n",
    "                    x = sm.add_constant(market_returns_aligned)\n",
    "\n",
    "                    # Perform linear regression\n",
    "                    model = sm.OLS(stock_returns_aligned, x).fit()\n",
    "\n",
    "                    # Store results in the dictionary\n",
    "                    capm_results[ticker] = model\n",
    "\n",
    "                    # Print the summary for the ticker\n",
    "                    print(f\"CAPM Results for {ticker}\")\n",
    "                    print(model.summary())\n",
    "                    print(\"\\n\" + \"-\" * 80 + \"\\n\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing CAPM for {ticker}: {e}\")\n",
    "\n",
    "    return capm_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62594155",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tech_tickers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Perform CAPM regression for the tickers\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m capm_results_tech \u001b[38;5;241m=\u001b[39m perform_capm(tech_tickers, market_data, output_dir_tech)\n\u001b[0;32m      3\u001b[0m capm_results_ev \u001b[38;5;241m=\u001b[39m perform_capm(tech_tickers, market_data, output_dir_ev)\n\u001b[0;32m      4\u001b[0m capm_results_finbank \u001b[38;5;241m=\u001b[39m perform_capm(tech_tickers, market_data, output_dir_finbank)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tech_tickers' is not defined"
     ]
    }
   ],
   "source": [
    "# Perform CAPM regression for the tickers\n",
    "capm_results_tech = perform_capm(tech_tickers, market_data, output_dir_tech)\n",
    "capm_results_ev = perform_capm(tech_tickers, market_data, output_dir_ev)\n",
    "capm_results_finbank = perform_capm(tech_tickers, market_data, output_dir_finbank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "52203a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output direction\n",
    "output_dir = \"./data/quantitative/CAPM_results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "cdf3a2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAPM regression summaries have been exported to ./data/quantitative/CAPM_results\\CAPM_Regression_Summaries_Tech.txt\n"
     ]
    }
   ],
   "source": [
    "# Export CAPM regression summaries to a text file\n",
    "output_summary_tech = os.path.join(output_dir, \"CAPM_Regression_Summaries_Tech.txt\")\n",
    "\n",
    "with open(output_summary_tech, \"w\") as file:\n",
    "    for ticker, model in capm_results_tech.items():\n",
    "        file.write(f\"CAPM Regression Summary for {ticker}:\\n\")\n",
    "        file.write(model.summary().as_text())  # Write the summary as text\n",
    "        file.write(\"\\n\" + \"-\" * 80 + \"\\n\")  # Add a separator for readability\n",
    "\n",
    "print(f\"CAPM regression summaries have been exported to {output_summary_tech}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "697ec046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAPM regression summaries have been exported to ./data/quantitative/CAPM_results\\CAPM_Regression_Summaries_Ev.txt\n"
     ]
    }
   ],
   "source": [
    "# Export CAPM regression summaries to a text file\n",
    "output_summary_ev = os.path.join(output_dir, \"CAPM_Regression_Summaries_Ev.txt\")\n",
    "\n",
    "with open(output_summary_ev, \"w\") as file:\n",
    "    for ticker, model in capm_results_tech.items():\n",
    "        file.write(f\"CAPM Regression Summary for {ticker}:\\n\")\n",
    "        file.write(model.summary().as_text())  # Write the summary as text\n",
    "        file.write(\"\\n\" + \"-\" * 80 + \"\\n\")  # Add a separator for readability\n",
    "\n",
    "print(f\"CAPM regression summaries have been exported to {output_summary_ev}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "893b853b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAPM regression summaries have been exported to ./data/quantitative/CAPM_results\\CAPM_Regression_Summaries_Finbank.txt\n"
     ]
    }
   ],
   "source": [
    "# Export CAPM regression summaries to a text file\n",
    "output_summary_finbank = os.path.join(output_dir, \"CAPM_Regression_Summaries_Finbank.txt\")\n",
    "\n",
    "with open(output_summary_finbank, \"w\") as file:\n",
    "    for ticker, model in capm_results_tech.items():\n",
    "        file.write(f\"CAPM Regression Summary for {ticker}:\\n\")\n",
    "        file.write(model.summary().as_text())  # Write the summary as text\n",
    "        file.write(\"\\n\" + \"-\" * 80 + \"\\n\")  # Add a separator for readability\n",
    "\n",
    "print(f\"CAPM regression summaries have been exported to {output_summary_finbank}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
